{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b43d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78744122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fl_mnist_implementation_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0dcdde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    '''expects images for each class in seperate dir, \n",
    "    e.g all digits in 0 class in the directory named 0 '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        im_gray_resized = cv2.resize(im_gray, (48,69), interpolation = cv2.INTER_AREA)\n",
    "        #image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(im_gray_resized/255)\n",
    "        labels.append(label)\n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2dbb8b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 100/1635\n",
      "[INFO] processed 200/1635\n",
      "[INFO] processed 300/1635\n",
      "[INFO] processed 400/1635\n",
      "[INFO] processed 500/1635\n",
      "[INFO] processed 600/1635\n",
      "[INFO] processed 700/1635\n",
      "[INFO] processed 800/1635\n",
      "[INFO] processed 900/1635\n",
      "[INFO] processed 1000/1635\n",
      "[INFO] processed 1100/1635\n",
      "[INFO] processed 1200/1635\n",
      "[INFO] processed 1300/1635\n",
      "[INFO] processed 1400/1635\n",
      "[INFO] processed 1500/1635\n",
      "[INFO] processed 1600/1635\n"
     ]
    }
   ],
   "source": [
    "#declear path to your mnist data folder\n",
    "img_path = r'D:\\Important Files\\Dataset Collection\\Image Classification\\Facial Expression\\CK+_Complete'\n",
    "\n",
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=100)\n",
    "\n",
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "988b71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=2, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c95a6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c262baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fca979d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79ef0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform', padding='same', input_shape=shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Conv2D(64, (3,3), kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(64, (3,3), kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Conv2D(128, (3,3), kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(128, (3,3), kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "064dfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 35\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04f86190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 28.659% | global_loss: 1.924581527709961\n",
      "comm_round: 1 | global_acc: 28.659% | global_loss: 1.9067637920379639\n",
      "comm_round: 2 | global_acc: 32.927% | global_loss: 1.8931190967559814\n",
      "comm_round: 3 | global_acc: 34.146% | global_loss: 1.8816132545471191\n",
      "comm_round: 4 | global_acc: 38.415% | global_loss: 1.8635369539260864\n",
      "comm_round: 5 | global_acc: 44.512% | global_loss: 1.8379396200180054\n",
      "comm_round: 6 | global_acc: 56.098% | global_loss: 1.7656843662261963\n",
      "comm_round: 7 | global_acc: 62.195% | global_loss: 1.6745696067810059\n",
      "comm_round: 8 | global_acc: 65.854% | global_loss: 1.6057696342468262\n",
      "comm_round: 9 | global_acc: 68.293% | global_loss: 1.5431164503097534\n",
      "comm_round: 10 | global_acc: 75.610% | global_loss: 1.4942086935043335\n",
      "comm_round: 11 | global_acc: 80.488% | global_loss: 1.447526454925537\n",
      "comm_round: 12 | global_acc: 82.317% | global_loss: 1.4511319398880005\n",
      "comm_round: 13 | global_acc: 85.366% | global_loss: 1.3706462383270264\n",
      "comm_round: 14 | global_acc: 86.585% | global_loss: 1.3354132175445557\n",
      "comm_round: 15 | global_acc: 91.463% | global_loss: 1.3151065111160278\n",
      "comm_round: 16 | global_acc: 92.073% | global_loss: 1.28256356716156\n",
      "comm_round: 17 | global_acc: 93.902% | global_loss: 1.2666789293289185\n",
      "comm_round: 18 | global_acc: 95.732% | global_loss: 1.2499221563339233\n",
      "comm_round: 19 | global_acc: 96.951% | global_loss: 1.2382023334503174\n",
      "comm_round: 20 | global_acc: 97.561% | global_loss: 1.2235621213912964\n",
      "comm_round: 21 | global_acc: 98.171% | global_loss: 1.218883991241455\n",
      "comm_round: 22 | global_acc: 96.951% | global_loss: 1.2150171995162964\n",
      "comm_round: 23 | global_acc: 98.171% | global_loss: 1.2073334455490112\n",
      "comm_round: 24 | global_acc: 97.561% | global_loss: 1.2056260108947754\n",
      "comm_round: 25 | global_acc: 98.171% | global_loss: 1.2004380226135254\n",
      "comm_round: 26 | global_acc: 98.171% | global_loss: 1.2030905485153198\n",
      "comm_round: 27 | global_acc: 98.171% | global_loss: 1.1981563568115234\n",
      "comm_round: 28 | global_acc: 98.171% | global_loss: 1.1979701519012451\n",
      "comm_round: 29 | global_acc: 98.171% | global_loss: 1.1933454275131226\n",
      "comm_round: 30 | global_acc: 98.171% | global_loss: 1.197652816772461\n",
      "comm_round: 31 | global_acc: 98.171% | global_loss: 1.1959038972854614\n",
      "comm_round: 32 | global_acc: 98.171% | global_loss: 1.198347806930542\n",
      "comm_round: 33 | global_acc: 100.000% | global_loss: 1.1862759590148926\n",
      "comm_round: 34 | global_acc: 100.000% | global_loss: 1.1831984519958496\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build((69, 48, 1), 7)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build((69, 48, 1), 7)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=3, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfa405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
